{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMtFG9w91Qd1S+hpehBrhpv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 0. 드라이브 연결"],"metadata":{"id":"_hBfwrg2AaPW"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"cnEOjaRJkWoR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. 기존 데이터셋 불러오는 코드"],"metadata":{"id":"3eAs9EVBAd0N"}},{"cell_type":"code","source":["import pandas as pd\n","\n","path = \"/content/drive/MyDrive/DILAB/Qwen2-7B-Instruct/results/original_1.3/original_1_3_results.parquet\"\n","\n","VIEW_NUM = 15\n","\n","df = pd.read_parquet(path)\n","\n","for idx, row in df.iterrows():\n","  print(f\"\\n📌======= {idx + 1}번 행 =======📌\")\n","\n","  for i, col in enumerate(df.columns, start=1):\n","    value = row[col]\n","\n","    print(f\"✅{i}. {col}\\n{value}\\n\")"],"metadata":{"id":"vUW08aDSHd-t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. 데이터 매핑"],"metadata":{"id":"0JdWUQZBAjOk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mefdBUckSOD"},"outputs":[],"source":["import pandas as pd\n","\n","path = \"/content/drive/MyDrive/DILAB/Qwen2-7B-Instruct/results/original_1.3/original_1_3_results.parquet\"\n","VIEW_NUM = 10\n","\n","df = pd.read_parquet(path)\n","\n","# 1. 출력할 컬럼 이름들만 리스트로 정의\n","columns_to_print = [\n","    'original_model_output',\n","    'trained_model_output',\n","    'original_bhc',\n","    'original_di'\n","]\n","\n","# 2. 원래 컬럼 번호를 가져오기 위해 딕셔너리 생성\n","col_indices = {col: df.columns.get_loc(col) + 1 for col in columns_to_print}\n","\n","# 3. VIEW_NUM 만큼만 상위 N개 행을 가져옴\n","for idx, row in df.head(VIEW_NUM).iterrows():\n","    print(f\"\\n📌======= {idx + 1}번 행 =======📌\")\n","\n","    # 4. 정의한 리스트(columns_to_print)에 있는 컬럼들만 반복\n","    for col_name in columns_to_print:\n","        value = row[col_name]\n","        original_index = col_indices[col_name]  # 저장해둔 원래 번호 가져오기\n","\n","        print(f\"✅{original_index}. {col_name}\\n{value}\\n\")"]},{"cell_type":"markdown","source":["# 2.1 데이터 분할 확인"],"metadata":{"id":"txC_vY6iAuQT"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","path = \"/content/drive/MyDrive/DILAB/Qwen2-7B-Instruct/results/original_1.3/original_1_3_results.parquet\"\n","VIEW_NUM = 5 # 확인할 행 개수 조절\n","\n","df = pd.read_parquet(path)\n","\n","# 텍스트 분리 함수\n","def split_bhc_di(text):\n","    if not isinstance(text, str):\n","        return \"\", \"\"\n","    bhc_pattern = r\"Brief Hospital Course\\s*\"\n","    di_pattern = r\"Discharge Instructions\\s*\"\n","    bhc_match = re.search(bhc_pattern, text, re.IGNORECASE)\n","    di_match = re.search(di_pattern, text, re.IGNORECASE)\n","    bhc_text = \"\"\n","    di_text = \"\"\n","\n","    if bhc_match and di_match:\n","        bhc_start = bhc_match.end()\n","        bhc_end = di_match.start()\n","        # BHC 텍스트 추출 후 맨 앞 콜론 및 관련 공백 제거\n","        bhc_text = text[bhc_start:bhc_end].strip().lstrip(':').strip()\n","        di_start = di_match.end()\n","        # DI 텍스트 추출 후 맨 앞 콜론 및 관련 공백 제거\n","        di_text = text[di_start:].strip().lstrip(':').strip()\n","    elif bhc_match:\n","        bhc_start = bhc_match.end()\n","        # BHC 텍스트 추출 후 맨 앞 콜론 및 관련 공백 제거\n","        bhc_text = text[bhc_start:].strip().lstrip(':').strip()\n","    elif di_match:\n","        di_start = di_match.end()\n","        # DI 텍스트 추출 후 맨 앞 콜론 및 관련 공백 제거\n","        di_text = text[di_start:].strip().lstrip(':').strip()\n","    return bhc_text, di_text\n","\n","# 상위 VIEW_NUM 개 행에 대해 분할 결과 출력\n","for idx, row in df.head(VIEW_NUM).iterrows():\n","    print(f\"\\n\\n\\n📌======= {idx + 1}번 행 분할 결과 확인 =======📌\")\n","\n","    # 원본 모델 출력 가져오기 및 분리\n","    om_output = row['original_model_output']\n","    om_bhc, om_di = split_bhc_di(om_output)\n","\n","    # 파인튜닝 모델 출력 가져오기 및 분리\n","    tm_output = row['trained_model_output']\n","    tm_bhc, tm_di = split_bhc_di(tm_output)\n","\n","    # 정답 가져오기\n","    gt_bhc = str(row['original_bhc']) if pd.notna(row['original_bhc']) else \"\"\n","    gt_di = str(row['original_di']) if pd.notna(row['original_di']) else \"\"\n","\n","    print(\"\\n--- [원본 모델 BHC] ---\")\n","    print(om_bhc)\n","\n","    print(\"\\n\\n--- [원본 모델 DI] ---\")\n","    print(om_di)\n","\n","    print(\"\\n\\n--- [파인튜닝 모델 BHC] ---\")\n","    print(tm_bhc)\n","\n","    print(\"\\n\\n--- [파인튜닝 모델 DI] ---\")\n","    print(tm_di)\n","\n","    print(\"\\n--- [정답 BHC] ---\")\n","    print(gt_bhc)\n","\n","    print(\"\\n--- [정답 DI] ---\")\n","    print(gt_di)\n","    print(\"=\" * 40) # 행 구분"],"metadata":{"id":"_A2A8s1mJtz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. 평가 (BERTScore)"],"metadata":{"id":"l5W0QTMAA2MO"}},{"cell_type":"code","source":["# 평가에 필요한 라이브러리 다운\n","!pip uninstall transformers bert-score -y\n","!pip install transformers bert-score"],"metadata":{"id":"0638_5emJdtO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from bert_score import score\n","import re\n","import warnings\n","import torch\n","import numpy as np # NaN 값을 사용하기 위해 numpy 임포트\n","\n","# bert_score 라이브러리 경고 메시지 무시\n","warnings.filterwarnings(\"ignore\", message=\"Some weights of RobertaModel were not initialized from the model checkpoint\")\n","\n","# --- 설정 ---\n","path = \"/content/drive/MyDrive/DILAB/Qwen2-7B-Instruct/results/original_1.3/original_1_3_results.parquet\"\n","VIEW_NUM = None # None으로 설정하면 전체 행 처리, 숫자로 지정하면 상위 N개 행만 처리\n","MODEL_TYPE = \"roberta-large\" # BERTScore 계산에 사용할 모델\n","# --- 설정 끝 ---\n","\n","# GPU 사용 가능 여부 확인 및 device 설정 (문자열로 변경)\n","if torch.cuda.is_available():\n","    device = \"cuda:0\" # GPU 사용 시 문자열 \"cuda:0\"\n","    print(\"Using device: GPU\")\n","else:\n","    device = \"cpu\"    # CPU 사용 시 문자열 \"cpu\"\n","    print(\"Using device: CPU\")\n","\n","\n","df = pd.read_parquet(path)\n","\n","# 텍스트 분리 함수 (맨 앞 콜론 제거 포함, 검증 완료)\n","def split_bhc_di(text):\n","    if not isinstance(text, str):\n","        return \"\", \"\"\n","    bhc_pattern = r\"Brief Hospital Course\\s*\"\n","    di_pattern = r\"Discharge Instructions\\s*\"\n","    bhc_match = re.search(bhc_pattern, text, re.IGNORECASE)\n","    di_match = re.search(di_pattern, text, re.IGNORECASE)\n","    bhc_text = \"\"\n","    di_text = \"\"\n","    if bhc_match and di_match:\n","        bhc_start = bhc_match.end()\n","        bhc_end = di_match.start()\n","        bhc_text = text[bhc_start:bhc_end].strip().lstrip(':').strip()\n","        di_start = di_match.end()\n","        di_text = text[di_start:].strip().lstrip(':').strip()\n","    elif bhc_match:\n","        bhc_start = bhc_match.end()\n","        bhc_text = text[bhc_start:].strip().lstrip(':').strip()\n","    elif di_match:\n","        di_start = di_match.end()\n","        di_text = text[di_start:].strip().lstrip(':').strip()\n","    return bhc_text, di_text\n","\n","# 결과를 저장할 리스트\n","bert_scores = []\n","\n","# 처리할 행 결정\n","if VIEW_NUM is None:\n","    rows_to_process = df.iterrows()\n","    total_rows = len(df)\n","else:\n","    rows_to_process = df.head(VIEW_NUM).iterrows()\n","    total_rows = VIEW_NUM\n","\n","print(f\"총 {total_rows}개 행에 대한 BERTScore 계산을 시작합니다 (모델: {MODEL_TYPE}, 장치: {device})...\")\n","\n","# 행 반복, 텍스트 분리, BERTScore 계산\n","for idx, row in rows_to_process:\n","    if (idx + 1) % 50 == 0: # 50행마다 진행 상황 출력\n","        print(f\"행 {idx + 1}/{total_rows} 처리 중...\")\n","\n","    # 모델 출력값 가져오기\n","    om_output = row['original_model_output']\n","    tm_output = row['trained_model_output']\n","\n","    # 정답(Ground Truth) 가져오기 (NaN 값 처리)\n","    gt_bhc = str(row['original_bhc']) if pd.notna(row['original_bhc']) else \"\"\n","    gt_di = str(row['original_di']) if pd.notna(row['original_di']) else \"\"\n","\n","    # 모델 출력값 분리\n","    om_bhc, om_di = split_bhc_di(om_output)\n","    tm_bhc, tm_di = split_bhc_di(tm_output)\n","\n","    # --- BERTScore 계산 (6가지 비교) ---\n","    # 점수 변수를 NaN으로 초기화\n","    f1_om_gt_bhc, f1_tm_gt_bhc, f1_om_gt_di, f1_tm_gt_di = np.nan, np.nan, np.nan, np.nan\n","    f1_om_tm_bhc, f1_om_tm_di = np.nan, np.nan\n","\n","    # 계산 함수 정의 (오류 처리 포함)\n","    def calculate_f1(candidates, references):\n","        # 함수 내부에서 빈 문자열 체크 제거 (호출 전에 체크)\n","        try:\n","            _, _, F1 = score(candidates, references, model_type=MODEL_TYPE, lang='en', verbose=False, device=device)\n","            return F1[0].item() if F1.numel() > 0 else 0.0 # 0.0 반환 유지 (오류 시)\n","        except Exception as e:\n","            print(f\" - 행 {idx + 1} 계산 오류: {e}\")\n","            return np.nan # 오류 발생 시 NaN 반환\n","\n","    # 점수 계산\n","    if om_bhc and gt_bhc:\n","        f1_om_gt_bhc = calculate_f1([om_bhc], [gt_bhc])\n","    if tm_bhc and gt_bhc:\n","        f1_tm_gt_bhc = calculate_f1([tm_bhc], [gt_bhc])\n","    if om_di and gt_di:\n","        f1_om_gt_di  = calculate_f1([om_di], [gt_di])\n","    if tm_di and gt_di:\n","        f1_tm_gt_di  = calculate_f1([tm_di], [gt_di])\n","    if om_bhc and tm_bhc: # OM vs TM 비교도 양쪽 다 있어야 계산\n","        f1_om_tm_bhc = calculate_f1([om_bhc], [tm_bhc])\n","    if om_di and tm_di: # OM vs TM 비교도 양쪽 다 있어야 계산\n","        f1_om_tm_di  = calculate_f1([om_di], [tm_di])\n","\n","\n","    # 결과 리스트에 추가\n","    bert_scores.append({\n","        'row_index': idx + 1,\n","        'F1_OM_vs_GT_BHC': f1_om_gt_bhc,\n","        'F1_TM_vs_GT_BHC': f1_tm_gt_bhc,\n","        'F1_OM_vs_GT_DI': f1_om_gt_di,\n","        'F1_TM_vs_GT_DI': f1_tm_gt_di,\n","        'F1_OM_vs_TM_BHC': f1_om_tm_bhc,\n","        'F1_OM_vs_TM_DI': f1_om_tm_di,\n","    })\n","\n","print(f\"총 {total_rows}개 행에 대한 BERTScore 계산 완료.\")\n","\n","# 결과를 데이터프레임으로 변환\n","results_df = pd.DataFrame(bert_scores)\n","\n","# 결과 출력 (상위 5개 행)\n","print(\"\\n===== BERTScore F1 결과 (상위 5개 행) =====\")\n","print(results_df.head()) # NaN 값이 포함된 채로 출력될 수 있음\n","\n","# 평균 점수 출력\n","print(\"\\n===== 평균 BERTScore F1 (NaN 제외) =====\")\n","avg_scores = results_df.mean(numeric_only=True) # skipna=True가 기본값\n","print(f\"원본 모델 BHC vs 정답 BHC: {avg_scores.get('F1_OM_vs_GT_BHC', 0.0):.4f}\")\n","print(f\"파인튜닝 모델 BHC vs 정답 BHC: {avg_scores.get('F1_TM_vs_GT_BHC', 0.0):.4f}\")\n","print(f\"원본 모델 DI vs 정답 DI: {avg_scores.get('F1_OM_vs_GT_DI', 0.0):.4f}\")\n","print(f\"파인튜닝 모델 DI vs 정답 DI: {avg_scores.get('F1_TM_vs_GT_DI', 0.0):.4f}\")\n","print(f\"원본 모델 BHC vs 파인튜닝 모델 BHC: {avg_scores.get('F1_OM_vs_TM_BHC', 0.0):.4f}\")\n","print(f\"원본 모델 DI vs 파인튜닝 모델 DI: {avg_scores.get('F1_OM_vs_TM_DI', 0.0):.4f}\")\n","\n","print(\"\\n===== 각 비교별 NaN (계산 건너뜀) 개수 =====\")\n","print(results_df.isnull().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjCNp1zXJdaC","outputId":"dde57539-a658-4ef2-8e5b-c774286d2f98","executionInfo":{"status":"ok","timestamp":1761182365052,"user_tz":-540,"elapsed":96102,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: GPU\n","총 10개 행에 대한 BERTScore 계산을 시작합니다 (모델: roberta-large, 장치: cuda:0)...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["총 10개 행에 대한 BERTScore 계산 완료.\n","\n","===== BERTScore F1 결과 (상위 5개 행) =====\n","   row_index  F1_OM_vs_GT_BHC  F1_TM_vs_GT_BHC  F1_OM_vs_GT_DI  \\\n","0          1         0.820352         0.854543        0.794991   \n","1          2         0.788898         0.823793        0.788979   \n","2          3         0.817770         0.829365        0.809633   \n","3          4         0.798389         0.802302        0.791982   \n","4          5         0.811931         0.850316        0.794196   \n","\n","   F1_TM_vs_GT_DI  F1_OM_vs_TM_BHC  F1_OM_vs_TM_DI  \n","0        0.832612         0.805765        0.792189  \n","1        0.834645         0.796798        0.784696  \n","2        0.887345         0.822171        0.829458  \n","3        0.844316         0.784360        0.809186  \n","4             NaN         0.805998             NaN  \n","\n","===== 평균 BERTScore F1 (NaN 제외) =====\n","원본 모델 BHC vs 정답 BHC: 0.8086\n","파인튜닝 모델 BHC vs 정답 BHC: 0.8415\n","원본 모델 DI vs 정답 DI: 0.8003\n","파인튜닝 모델 DI vs 정답 DI: 0.8564\n","원본 모델 BHC vs 파인튜닝 모델 BHC: 0.8082\n","원본 모델 DI vs 파인튜닝 모델 DI: 0.8018\n","\n","===== 각 비교별 NaN (계산 건너뜀) 개수 =====\n","row_index          0\n","F1_OM_vs_GT_BHC    1\n","F1_TM_vs_GT_BHC    1\n","F1_OM_vs_GT_DI     1\n","F1_TM_vs_GT_DI     1\n","F1_OM_vs_TM_BHC    1\n","F1_OM_vs_TM_DI     2\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["# 3.1 시각화"],"metadata":{"id":"Wn2VkDX_EInj"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","if 'results_df' not in locals():\n","    print(\"오류: 'results_df' 데이터프레임이 존재하지 않습니다.\")\n","else:\n","    print(\"시각화 진행 중...\")\n","    # --- 시각화 ---\n","\n","    # 그래프 스타일 설정\n","    sns.set_style(\"whitegrid\")\n","\n","    # 1행 2열의 그래프 영역 생성 (BHC용, DI용)\n","    fig, axes = plt.subplots(1, 2, figsize=(14, 7)) # 가로 크기 늘림\n","\n","    # 1. BHC 점수 비교 상자 그림 (OM vs GT, TM vs GT)\n","    # BHC 비교에 사용할 컬럼만 선택\n","    bhc_scores_to_plot = results_df[['F1_OM_vs_GT_BHC', 'F1_TM_vs_GT_BHC']]\n","    # 그래프 레이블을 위한 컬럼 이름 변경\n","    bhc_scores_to_plot.columns = ['Original vs. GT', 'Fine-tuned vs. GT']\n","\n","    sns.boxplot(data=bhc_scores_to_plot, ax=axes[0], palette=\"pastel\") # 색상 팔레트 변경\n","    axes[0].set_title('BHC: BERTScore F1 Comparison (vs. Ground Truth)') # 제목 수정\n","    axes[0].set_ylabel('BERTScore F1')\n","    axes[0].set_ylim(0, 1) # F1 점수 범위 (0 ~ 1)\n","\n","    # 2. DI 점수 비교 상자 그림 (OM vs GT, TM vs GT)\n","    # DI 비교에 사용할 컬럼만 선택\n","    di_scores_to_plot = results_df[['F1_OM_vs_GT_DI', 'F1_TM_vs_GT_DI']]\n","    # 컬럼 이름 변경\n","    di_scores_to_plot.columns = ['Original vs. GT', 'Fine-tuned vs. GT']\n","\n","    sns.boxplot(data=di_scores_to_plot, ax=axes[1], palette=\"pastel\")\n","    axes[1].set_title('DI: BERTScore F1 Comparison (vs. Ground Truth)') # 제목 수정\n","    axes[1].set_ylabel('BERTScore F1')\n","    axes[1].set_ylim(0, 1)\n","\n","    # 그래프 레이아웃 조정 및 파일 저장\n","    plt.tight_layout()\n","    plt.savefig(\"bertscore_vs_gt_comparison_boxplot.png\") # 파일 이름 변경\n","\n","    # 저장 완료 메시지 출력\n","    print(\"상자 그림이 bertscore_vs_gt_comparison_boxplot.png 로 저장되었습니다.\")\n","\n","    # plt.show()"],"metadata":{"id":"jBZnpUP_EOEv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. 평가 (BLEU)"],"metadata":{"id":"ZyBTiXtwDJMA"}},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"c6jRlP6GEuMe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 필요 라이브러리 설치 (Colab에서 처음 실행 시 필요)\n","# !pip install nltk\n","\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","import numpy as np # NaN 값을 사용하기 위해 numpy 임포트\n","\n","# nltk 데이터 다운로드 (punkt 토크나이저)\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","\n","# --- 설정 ---\n","path = \"/content/drive/MyDrive/DILAB/Qwen2-7B-Instruct/results/original_1.3/original_1_3_results.parquet\"\n","VIEW_NUM = None # None으로 설정하면 전체 행 처리, 숫자로 지정하면 상위 N개 행만 처리\n","\n","df = pd.read_parquet(path)\n","\n","# 텍스트 분리 함수\n","def split_bhc_di(text):\n","    if not isinstance(text, str):\n","        return \"\", \"\"\n","    bhc_pattern = r\"Brief Hospital Course\\s*\"\n","    di_pattern = r\"Discharge Instructions\\s*\"\n","    bhc_match = re.search(bhc_pattern, text, re.IGNORECASE)\n","    di_match = re.search(di_pattern, text, re.IGNORECASE)\n","    bhc_text = \"\"\n","    di_text = \"\"\n","    if bhc_match and di_match:\n","        bhc_start = bhc_match.end()\n","        bhc_end = di_match.start()\n","        bhc_text = text[bhc_start:bhc_end].strip().lstrip(':').strip()\n","        di_start = di_match.end()\n","        di_text = text[di_start:].strip().lstrip(':').strip()\n","    elif bhc_match:\n","        bhc_start = bhc_match.end()\n","        bhc_text = text[bhc_start:].strip().lstrip(':').strip()\n","    elif di_match:\n","        di_start = di_match.end()\n","        di_text = text[di_start:].strip().lstrip(':').strip()\n","    return bhc_text, di_text\n","\n","# 결과를 저장할 리스트\n","bleu_scores_list = []\n","\n","# 처리할 행 결정\n","if VIEW_NUM is None:\n","    rows_to_process = df.iterrows()\n","    total_rows = len(df)\n","else:\n","    rows_to_process = df.head(VIEW_NUM).iterrows()\n","    total_rows = VIEW_NUM\n","\n","print(f\"총 {total_rows}개 행에 대한 BLEU-4 점수 계산을 시작합니다...\")\n","\n","# Smoothing function 정의\n","chencherry = SmoothingFunction()\n","\n","# 토크나이징 및 점수 계산 함수 정의\n","def calculate_bleu(hypothesis_str, reference_str):\n","    # 함수 내부 빈 문자열 체크 제거 (호출 전에 체크)\n","    hypothesis_tokens = hypothesis_str.split()\n","    reference_tokens_list = [reference_str.split()]\n","    try:\n","        score_val = sentence_bleu(reference_tokens_list, hypothesis_tokens, smoothing_function=chencherry.method1)\n","        return score_val\n","    except Exception as e:\n","        print(f\" - 행 계산 중 오류 발생 (BLEU): {e}\") # 오류 발생 시 행 번호 명시적으로 출력하지 않음\n","        return np.nan # 오류 시 NaN 반환\n","\n","# 행 반복, 텍스트 분리, BLEU 점수 계산\n","for idx, row in rows_to_process:\n","    if (idx + 1) % 50 == 0: # 50행마다 진행 상황 출력\n","        print(f\"행 {idx + 1}/{total_rows} 처리 중...\")\n","\n","    # 모델 출력값 가져오기\n","    om_output = row['original_model_output']\n","    tm_output = row['trained_model_output']\n","\n","    # 정답(Ground Truth) 가져오기 (NaN 값 처리)\n","    gt_bhc = str(row['original_bhc']) if pd.notna(row['original_bhc']) else \"\"\n","    gt_di = str(row['original_di']) if pd.notna(row['original_di']) else \"\"\n","\n","    # 모델 출력값 분리\n","    om_bhc, om_di = split_bhc_di(om_output)\n","    tm_bhc, tm_di = split_bhc_di(tm_output)\n","\n","    # --- BLEU 점수 계산 ---\n","    bleu_om_gt_bhc, bleu_tm_gt_bhc, bleu_om_gt_di, bleu_tm_gt_di = np.nan, np.nan, np.nan, np.nan\n","\n","    # 점수 계산\n","    if om_bhc and gt_bhc:\n","        bleu_om_gt_bhc = calculate_bleu(om_bhc, gt_bhc)\n","    if tm_bhc and gt_bhc:\n","        bleu_tm_gt_bhc = calculate_bleu(tm_bhc, gt_bhc)\n","    if om_di and gt_di:\n","        bleu_om_gt_di  = calculate_bleu(om_di, gt_di)\n","    if tm_di and gt_di:\n","        bleu_tm_gt_di  = calculate_bleu(tm_di, gt_di)\n","\n","    # 결과 리스트에 추가\n","    bleu_scores_list.append({\n","        'row_index': idx + 1,\n","        'BLEU4_OM_vs_GT_BHC': bleu_om_gt_bhc,\n","        'BLEU4_TM_vs_GT_BHC': bleu_tm_gt_bhc,\n","        'BLEU4_OM_vs_GT_DI': bleu_om_gt_di,\n","        'BLEU4_TM_vs_GT_DI': bleu_tm_gt_di,\n","    })\n","\n","print(f\"총 {total_rows}개 행에 대한 BLEU-4 점수 계산 완료.\")\n","\n","# 결과를 데이터프레임으로 변환\n","bleu_results_df = pd.DataFrame(bleu_scores_list)\n","\n","# 결과 출력 (상위 5개 행)\n","print(\"\\n===== BLEU-4 점수 결과 (상위 5개 행) =====\")\n","print(bleu_results_df.head()) # NaN 값이 포함된 채로 출력될 수 있음\n","\n","# 평균 점수 출력\n","print(\"\\n===== 평균 BLEU-4 점수 (NaN 제외) =====\")\n","avg_bleu_scores = bleu_results_df.mean(numeric_only=True) # skipna=True가 기본값\n","print(f\"원본 모델 BHC vs 정답 BHC: {avg_bleu_scores.get('BLEU4_OM_vs_GT_BHC', 0.0):.4f}\")\n","print(f\"파인튜닝 모델 BHC vs 정답 BHC: {avg_bleu_scores.get('BLEU4_TM_vs_GT_BHC', 0.0):.4f}\")\n","print(f\"원본 모델 DI vs 정답 DI: {avg_bleu_scores.get('BLEU4_OM_vs_GT_DI', 0.0):.4f}\")\n","print(f\"파인튜닝 모델 DI vs 정답 DI: {avg_bleu_scores.get('BLEU4_TM_vs_GT_DI', 0.0):.4f}\")\n","\n","print(\"\\n===== 각 비교별 NaN (계산 건너뜀) 개수 =====\")\n","print(bleu_results_df.isnull().sum())\n"],"metadata":{"id":"sSFRlg75c5MZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761182207088,"user_tz":-540,"elapsed":193,"user":{"displayName":"Di Lab","userId":"16690045528125560783"}},"outputId":"e73bb5b5-f181-40fc-d6fd-853ebed3456a"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["총 10개 행에 대한 BLEU-4 점수 계산을 시작합니다...\n","총 10개 행에 대한 BLEU-4 점수 계산 완료.\n","\n","===== BLEU-4 점수 결과 (상위 5개 행) =====\n","   row_index  BLEU4_OM_vs_GT_BHC  BLEU4_TM_vs_GT_BHC  BLEU4_OM_vs_GT_DI  \\\n","0          1            0.009228            0.177181           0.004766   \n","1          2            0.003957            0.008319           0.002432   \n","2          3            0.004316            0.174411           0.003733   \n","3          4            0.001810            0.063336           0.005457   \n","4          5            0.000999            0.001277           0.001544   \n","\n","   BLEU4_TM_vs_GT_DI  \n","0           0.001604  \n","1           0.013378  \n","2           0.141416  \n","3           0.005420  \n","4                NaN  \n","\n","===== 평균 BLEU-4 점수 (NaN 제외) =====\n","원본 모델 BHC vs 정답 BHC: 0.0042\n","파인튜닝 모델 BHC vs 정답 BHC: 0.0795\n","원본 모델 DI vs 정답 DI: 0.0066\n","파인튜닝 모델 DI vs 정답 DI: 0.0413\n","\n","===== 각 비교별 NaN (계산 건너뜀) 개수 =====\n","row_index             0\n","BLEU4_OM_vs_GT_BHC    1\n","BLEU4_TM_vs_GT_BHC    1\n","BLEU4_OM_vs_GT_DI     1\n","BLEU4_TM_vs_GT_DI     1\n","dtype: int64\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","if 'bleu_results_df' not in locals():\n","    print(\"오류: 'bleu_results_df' 데이터프레임이 존재하지 않습니다.\")\n","else:\n","    print(\"BLEU-4 점수 시각화 진행 중...\")\n","    # --- 시각화 ---\n","\n","    # 그래프 스타일 설정\n","    sns.set_style(\"whitegrid\")\n","\n","    # 1행 2열의 그래프 영역 생성 (BHC용, DI용)\n","    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n","\n","    # 1. BHC 점수 비교 상자 그림 (OM vs GT, TM vs GT)\n","    # BHC 비교에 사용할 컬럼만 선택\n","    bhc_bleu_scores_to_plot = bleu_results_df[['BLEU4_OM_vs_GT_BHC', 'BLEU4_TM_vs_GT_BHC']]\n","    # 그래프 레이블을 위한 컬럼 이름 변경\n","    bhc_bleu_scores_to_plot.columns = ['Original vs. GT', 'Fine-tuned vs. GT']\n","\n","    sns.boxplot(data=bhc_bleu_scores_to_plot, ax=axes[0], palette=\"pastel\")\n","    axes[0].set_title('BHC: BLEU-4 Score Comparison (vs. Ground Truth)')\n","    axes[0].set_ylabel('BLEU-4 Score')\n","    # BLEU 점수는 보통 낮으므로, 상한선을 1.0 대신 데이터 최대값 기준으로 자동 조절하거나\n","    # 데이터 분포를 보고 적절히 설정 (예: 0.2 또는 0.3)\n","    axes[0].set_ylim(0, 1)\n","\n","    # 2. DI 점수 비교 상자 그림 (OM vs GT, TM vs GT)\n","    # DI 비교에 사용할 컬럼만 선택\n","    di_bleu_scores_to_plot = bleu_results_df[['BLEU4_OM_vs_GT_DI', 'BLEU4_TM_vs_GT_DI']]\n","    # 컬럼 이름 변경\n","    di_bleu_scores_to_plot.columns = ['Original vs. GT', 'Fine-tuned vs. GT']\n","\n","    sns.boxplot(data=di_bleu_scores_to_plot, ax=axes[1], palette=\"pastel\")\n","    axes[1].set_title('DI: BLEU-4 Score Comparison (vs. Ground Truth)')\n","    axes[1].set_ylabel('BLEU-4 Score')\n","    axes[1].set_ylim(0, 1) # y축 범위 설정\n","\n","    # 그래프 레이아웃 조정 및 파일 저장\n","    plt.tight_layout()\n","    plt.savefig(\"bleu4_comparison_boxplot.png\") # 파일 이름 지정\n","\n","    # 저장 완료 메시지 출력\n","    print(\"상자 그림이 bleu4_comparison_boxplot.png 로 저장되었습니다.\")\n","\n","    plt.show()"],"metadata":{"id":"LpGGkFfNFQPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WtHlPvmFFQdk"},"execution_count":null,"outputs":[]}]}