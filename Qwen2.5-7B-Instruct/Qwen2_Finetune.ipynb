{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyO4Sdj53nODQ/pLqJ8/NaXP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["print(\"모든 필수 라이브러리를 최신 버전으로 설치\")\n","!pip install -q -U transformers accelerate peft trl datasets pandas bitsandbytes"],"metadata":{"id":"rV0gER3DqwKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 라이브러리 임포트 및 환경 설정 ---\n","print(\"\\n 라이브러리 불러오기\")\n","\n","import pandas as pd\n","import torch\n","from datasets import Dataset, DatasetDict\n","from peft import LoraConfig\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n",")\n","from trl import SFTTrainer\n","import numpy as np\n","from huggingface_hub import notebook_login\n","from google.colab import drive\n","import os\n","\n","os.environ[\"WANDB_DISABLED\"] = \"true\" # Weights & Biases 로깅 기능 비활성화\n","\n","drive.mount('/content/drive') # 드라이브 연결\n","notebook_login() # Hugging Face Hub 로그인\n","\n","# --- 데이터 로드 및 전처리 ---\n","file_path = '/content/drive/MyDrive/DILAB/MARS/mimic-iv-note_2.2/files/note/discharge.csv'\n","n_rows_to_read = 2000 # 상위 2000개의 행만 읽음\n","df = pd.read_csv(file_path, usecols=['subject_id', 'text'], nrows=n_rows_to_read)\n","df.dropna(subset=['text'], inplace=True)  # \"text\" 컬럼이 NaN일 경우, 해당 행 제거\n","\n","unique_user_ids = df['subject_id'].unique() # ID 목록을 8:1:1 비율로 분할할 인덱스를 계산\n","np.random.shuffle(unique_user_ids)\n","train_split = int(len(unique_user_ids) * 0.8)\n","dev_split = int(len(unique_user_ids) * 0.9)\n","train_ids, dev_ids, test_ids = np.split(unique_user_ids, [train_split, dev_split])\n","train_df = df[df['subject_id'].isin(train_ids)]\n","dev_df = df[df['subject_id'].isin(dev_ids)]\n","\n","# Hugging Face의 datasets 라이브러리가 사용하기 좋은 형태로 변환\n","raw_datasets = DatasetDict({\n","    \"train\": Dataset.from_pandas(train_df),\n","    \"validation\": Dataset.from_pandas(dev_df)\n","})\n","print(\"데이터 분할 완료\")\n","\n","# SFTTrainer가 학습할 최종 텍스트 형태(프롬프트)를 만드는 함수를 정의]\n","def create_prompt(example):\n","    # 'example'은 데이터 한 개(row)\n","    # 'text' 컬럼의 내용을 가져와 프롬프트 형식으로 만듦\n","    return {\n","        'text': f\"\"\"### Instruction:\n","Summarize the key points from the following medical discharge summary.\n","\n","### Input:\n","{example['text']}\n","\n","### Response:\n","\"\"\"\n","    }\n","\n","# .map() 함수를 사용하여 데이터셋의 모든 행에 create_prompt 함수를 미리 적용\n","# SFTTrainer는 기본적으로 text라는 이름의 컬럼을 찾기 때문에, 이 과정을 통해 원본 text 컬럼이 학습에 적합한 프롬프트가 담긴 새로운 text 컬럼으로 대체\n","column_names = raw_datasets['train'].column_names\n","formatted_datasets = raw_datasets.map(create_prompt, remove_columns=column_names)\n","print(\"\\n데이터 포맷팅 완료\")\n","\n","\n","# --- 모델 및 QLoRA 설정 ---\n","print(\"\\n모델과 토크나이저를 로드\")\n","model_name = \"Qwen/Qwen2-7B-Instruct\"\n","# 4비트 양자화(Quantization) 설정을 정의 (QLoRA의 'Q' 부분)\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n","# LoRA(Low-Rank Adaptation) 어댑터 설정을 정의 (QLoRA의 'LoRA' 부분)\n","peft_config = LoraConfig(lora_alpha=16, lora_dropout=0.1, r=64, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"])\n","\n","# 위에서 정의한 양자화 설정을 적용하여 모델을 로드합니다.\n","model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n","model.config.use_cache = False\n","# 모델에 맞는 토크나이저를 로드\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","print(\"모델 설정 완료\")\n","\n","\n","# --- 학습 실행 ---\n","print(\"\\n파인튜닝 시작\")\n","training_arguments = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/MyModels/Qwen2_Finetune_Results\", # 체크포인트 등 결과물이 저장될 경로\n","    num_train_epochs=1,                # 전체 데이터셋을 1번 학습\n","    per_device_train_batch_size=1,     # 한 번에 GPU에 올릴 데이터 샘플 수\n","    gradient_accumulation_steps=4,     # 4번의 스텝마다 그래디언트를 업데이트 (배치 사이즈 1*4=4 효과)\n","    optim=\"paged_adamw_32bit\",         # 메모리 효율적인 AdamW 옵티마이저\n","    logging_steps=10,                  # 10 스텝마다 로그(loss 등) 출력\n","    learning_rate=2e-4,                # 학습률\n","    bf16=True,                         # A100/L4 GPU에서 성능을 높여주는 bfloat16 활성화\n","    max_grad_norm=0.3,                 # 그래디언트 클리핑 값\n","    warmup_ratio=0.03,                 # 학습 초반에 학습률을 서서히 증가시키는 구간 비율\n","    lr_scheduler_type=\"constant\",      # 학습률을 일정하게 유지\n",")\n","\n","# SFTTrainer에 불필요한 인자를 모두 제거하고,\n","# 미리 전처리된 데이터셋만 전달하여 가장 안정적으로 동작하도록 함\n","trainer = SFTTrainer(\n","    model=model,                       # 학습할 모델\n","    args=training_arguments,           # 학습 인자\n","    train_dataset=formatted_datasets[\"train\"], # 훈련 데이터셋 (이미 'text' 컬럼만 있음)\n","    eval_dataset=formatted_datasets[\"validation\"], # 평가 데이터셋\n","    peft_config=peft_config,           # LoRA 설정\n",")\n","\n","trainer.train()\n","\n","# --- 모델 저장 ---\n","print(\"\\n학습된 모델을 저장\")\n","# 학습된 LoRA 어댑터(튜닝 칩)만 지정된 경로에 저장\n","tuned_model_path = \"/content/drive/MyDrive/MyModels/Qwen2_Finetuned_Adapter\"\n","trainer.model.save_pretrained(tuned_model_path)\n","print(f\"모델 어댑터가 '{tuned_model_path}'에 저장됨\")"],"metadata":{"id":"YdfMizqZfyUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 라이브러리 임포트 ---\n","import torch\n","# PEFT(LoRA)로 파인튜닝된 모델을 쉽게 불러오기 위한 클래스\n","from peft import AutoPeftModelForCausalLM\n","# 텍스트를 토큰으로 변환하는 토크나이저를 불러오기 위한 클래스\n","from transformers import AutoTokenizer\n","\n","# --- 경로 설정 ---\n","# 구글 드라이브에 저장해 둔, 학습이 완료된 'LoRA 어댑터'의 경로\n","tuned_model_path = \"/content/drive/MyDrive/MyModels/Qwen2_Finetuned_Adapter\"\n","# 토크나이저는 파인튜닝 과정에서 변경되지 않았으므로,\n","# 원본 모델이 있던 허깅페이스 경로를 그대로 사용\n","base_model_name = \"Qwen/Qwen2-7B-Instruct\"\n","\n","print(\"파인튜닝된 모델과 원본 토크나이저를 로드\")\n","\n","# --- 모델과 토크나이저 불러오기 ---\n","# AutoPeftModelForCausalLM.from_pretrained()는 두 가지 일을 동시에 수행:\n","#   1. 'base_model_name'에 해당하는 원본 Qwen2 모델을 허깅페이스에서 다운로드/로드\n","#   2. 'tuned_model_path'에 저장된 LoRA 어댑터(튜닝 칩)를 그 위에 합침\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    tuned_model_path,\n","    device_map=\"auto\",          # 사용 가능한 GPU에 모델을 자동으로 할당\n","    torch_dtype=torch.bfloat16  # bfloat16 데이터 타입을 사용하여 메모리 효율성과 속도 향상\n",")\n","\n","# 토크나이저는 어댑터 폴더가 아닌, 원본 모델의 허깅페이스 경로에서 불러옴\n","# 어댑터 폴더에는 토크나이저 정보가 없기 때문\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","print(\"모델 로딩 완료\")\n","\n","# --- 3. 테스트용 데이터(프롬프트) 준비 ---\n","# 모델에게 요약시킬 새로운 의료 기록 텍스트\n","test_summary = \"\"\"\n","Dear Ms. ___,\n","It was a pleasure taking care of you! You came to us with\n","stomach pain and worsening distension. While you were here we\n","did a paracentesis to remove 1.5L of fluid from your belly. We\n","also placed you on you 40 mg of Lasix and 50 mg of Aldactone to\n","help you urinate the excess fluid still in your belly. As we\n","discussed, everyone has a different dose of lasix required to\n","make them urinate and it's likely that you weren't taking a high\n","enough dose. Please take these medications daily to keep excess\n","fluid off and eat a low salt diet. You will follow up with Dr.\n","___ in liver clinic and from there have your colonoscopy\n","and EGD scheduled. Of course, we are always here if you need us.\n","We wish you all the best!\n","Your ___ Team.\n","\"\"\"\n","\n","# 모델이 학습했을 때 사용했던 프롬프트 양식을 \"반드시\" 똑같이 사용해야 함\n","prompt = f\"\"\"### Instruction:\n","Summarize the key points from the following medical discharge summary.\n","\n","### Input:\n","{test_summary}\n","\n","### Response:\n","\"\"\"\n","\n","# --- 추론(Inference) 실행 ---\n","print(\"\\n추론 시작\")\n","\n","# 1. 텍스트 프롬프트를 토크나이저를 사용해 모델이 이해할 수 있는 숫자(ID)의 배열로 변환\n","# 2. return_tensors=\"pt\": 결과를 PyTorch 텐서 형태로 받음\n","# 3. .cuda(): 변환된 텐서를 GPU 메모리로 보냄\n","input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n","\n","# model.generate() 함수를 호출하여 텍스트 생성을 시작합니다.\n","outputs = model.generate(\n","    input_ids=input_ids,          # 입력 텍스트\n","    max_new_tokens=100,           # 최대 몇 개의 토큰(단어)을 새로 생성할지 지정\n","    do_sample=True,               # 샘플링 방식을 사용하여 좀 더 창의적인 텍스트 생성\n","    top_p=0.9,                    # 확률이 높은 상위 90%의 단어들 중에서만 샘플링\n","    temperature=0.7               # 값이 낮을수록 예측 가능한 답변, 높을수록 다양한 답변 생성 (0.7은 약간의 창의성 부여)\n",")\n","\n","# --- 결과 후처리 및 출력 ---\n","# 생성된 숫자(토큰) 배열을 다시 사람이 읽을 수 있는 텍스트로 변환(디코딩)\n","# skip_special_tokens=True: <|endoftext|> 같은 특수 토큰은 제외하고 출력\n","result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# 전체 생성된 텍스트에서 우리가 입력한 프롬프트 부분을 제외하고,\n","# 순수하게 모델이 생성한 'Response' 부분만 추출하여 깔끔하게 만듦\n","response_text = result.split(\"### Response:\")[1].strip()\n","\n","# 최종 결과를 출력\n","print(\"\\n\" + \"=\"*50)\n","print(\"파인튜닝된 모델의 요약 결과:\")\n","print(response_text)\n","print(\"=\"*50)"],"metadata":{"id":"yA4mCj9blT3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B3VALVnDvhhq"},"execution_count":null,"outputs":[]}]}